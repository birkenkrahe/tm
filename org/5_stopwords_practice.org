#+TITLE: Text mining in practice - Bag of Words - stopwords - PRACTICE
#+AUTHOR: [yourname]
#+SUBTITLE: Digital Humanities DSC 105 Spring 2023
#+STARTUP:overview hideblocks indent inlineimages
#+OPTIONS: toc:nil num:nil ^:nil
#+PROPERTY: header-args:R :session *R* :results output :exports both :noweb yes
* README

This lecture closely follows the 3rd part of the DataCamp lesson
"Jumping into Text Minin with Bag-of-Words" by Ted Kwartler, part of
his course on [[https://campus.datacamp.com/courses/text-mining-with-bag-of-words-in-r/]["Text Mining with Bag-of-Words in R"]].

* TODO Identify and pledge yourself

1) In Emacs, replace the placeholder ~[yourname]~ at the top of this
   file by your own name and write ~(pledged)~ next to it
2) Go with the cursor on the headline and hange the ~TODO~ label to ~DONE~
   by entering ~S-<right>~ ("Shift + right-arrow").

* Getting the coffee data

Run this in case you had to interrupt the previous session and don't
have the data in your R session:
#+begin_src R :results silent
  library(tm)
  coffee_df <- read.csv("../data/coffee.csv") # dataframe
  coffee_vec <- coffee_df$text # vector
  coffee_src <- VectorSource(coffee_vec) # source
  coffee_corpus <- VCorpus(coffee_src)
#+end_src

* All about stop words

- Load the ~tm~ package and look for the ~stopwords~ function:
  #+begin_src R
    library(tm)
    ## is stopwords any of the functions in tm?
    ... # store all function names in f_tm
    ... # check every function against "stopwords"
  #+end_src

- The function ~any~ is very useful: it checks if any of its arguments
  are true:
  #+begin_src R
    ...
  #+end_src

- Check out the ~stopwords~ in English ("en" or "english"), Spanish
  ("es"), German ("de" or "german").
  #+begin_src R
    ...
  #+end_src

- Check yourself if the word "should" is in ~stopwords("en")~:
  #+begin_src R
    ...
  #+end_src

- Add two stop words to ~stopwords("en")~ and check that they were added:
  1) append "word1" and "word2" to ~stopwords("en")~ using ~c()~
  2) store the result in ~all_stops~
  3) display the first two entries of ~all_stops~
  #+begin_src R
    ...
    ...
  #+end_src

- List the arguments of ~removeWords~.
  #+begin_src R

  #+end_src

* Exercise with ~stopwords~

- Remove all ~stopwords~ from sample ~text~, add two words to the standard
  ~stopwords~ dictionary, and remove them from ~text~, too.

- Define sample ~text~ vector.
  #+begin_src R
    text <-
      "<b>She</b> woke up at       6 A.M. It\'s so
       early!  She was only 10% awake and began drinking
       coffee in front of her computer."
    text
  #+end_src

- Remove "en" stopwords from ~text~ with ~removeWord~.
  #+begin_src R
    text
    ...
  #+end_src

- Add "coffee" and "bean" to the standard stop words and assign the
  result to ~new_stops~. Check that they are in ~new_stops~!
  #+begin_src R

  #+end_src

- Wait a moment! What if these words were already in ~stopwords~?
  1) save ~stopwords("en")~ as ~old_stops~
  2) check if any elements of ~old_stops~ are "coffee" or "bean"
  3) check if any elements of ~new_stops~ are "coffee" or "bean"
  #+begin_src R
    ... # store old stopwords in old_stops
    ...
    ...
  #+end_src

- Remove the customized stopwords, ~new_stops~, from ~text~:
  #+begin_src R
    text
    ...
  #+end_src

* Finding a string in a dataset

- To find a tweet in ~coffee_vec~ that contains both words, we need a
  few more tricks: index vectors with ~which~ and pattern search with
  ~grepl~.

- ~which~ runs its ~logical~ argument a vector and returns the indices
  that satisfy the logical argument:
  #+begin_src R
    ...
  #+end_src

- The same thing works with ~character~ vectors:
  #+begin_src R

  #+end_src

- It also works with ~stopwords~: e.g. is "cannot" in the ~stopwords~
  vector?
  #+begin_src R

  #+end_src

- ~grepl~ checks if its ~pattern~ is contained in a dataset ~x~. It returns
  a ~logical~ vector, a match or not for each element of ~x~:
  #+begin_src R

  #+end_src

- For example: check if any coffee tweets contain the word "Ramadan"
  #+begin_src R

  #+end_src

- Combine ~grepl~ and ~which~ to extract the corresponding index:
  #+begin_src R

  #+end_src

- Then print the corresponding tweets:
  #+begin_src R

  #+end_src

* Finding certain tweets in ~coffee_vec~

- Now, to find the tweets in ~coffee_vec~ that contain "coffee" AND
  "beans":
  1) create an index vector of tweets that contain "beans"
  2) store these tweets in ~bean~
  3) create an index vector of ~bean~ tweets that contain "coffee"
  4) store these tweets in ~coffee~

  #+begin_src R

  #+end_src

- Now re-run the code above to remove "bean" and "coffee" from the
  selection ~coffee_bean~:
  #+begin_src R

  #+end_src

* Word stemming on a sentence

- If you call ~stemDocument~ on a sentence it fails. Try it with the
  sample text:
  #+begin_src R
    sentence <- "In a complicated haste,
      Tom rushed to fix a new complication,
      too complicatedly."
    sentence
  #+end_src

- Alas, I wrote this over several lines and it contains newline
  characters ~\n~ - white space - do you know how to remove it?
  #+begin_src R

  #+end_src

- Now run ~stemDocument~ on the ~sentence~:
  #+begin_src R

  #+end_src

- This happens because ~stemDocument()~ treats the whole sentence *as one
  word*: the document is a ~character~ vector of length 1:
  #+begin_src R

  #+end_src

* Interlude: Splitting strings with ~strsplit~

- Call the help on ~strsplit~ in the ~base~ package:
  #+begin_src R

  #+end_src

- What are the arguments of ~strsplit~?
  #+begin_src R

  #+end_src

- For example, split this sentence: "Split this sentence" using ~""~ as
  the ~split~ argument and save the result to ~foo~:
  #+begin_src R

  #+end_src

- What's the correct ~split~ to get the words? Save the result in ~bar~:
  #+begin_src R

  #+end_src

- Now, the result of the split is a ~list~ and needs to be un-listed:
  #+begin_src R

  #+end_src

- Just for fun, can you turn the pipeline in the last code block into
  a nested statement?
  #+begin_src R

  #+end_src

* Stem and re-complete a sentence

- Sample sentence and sample dictionary for stem re-completion:
  #+begin_src R
    sentence <- stripWhitespace("In a complicated haste,
                                Tom rushed to fix a new complication,
                                too complicatedly.")
    sentence
    comp_dict <- c("In","a","complicate","haste",
                   "Tom","rush","to","fix","new","too")
    comp_dict
  #+end_src

- Remove the punctuation marks in ~sentence~ using ~removePunctuation()~,
  and assign the result to ~foo~:
  #+begin_src R

  #+end_src

- Call ~strsplit()~ on ~foo~ with the split argument set equal to " ", and
  save the result to ~bar~:
  #+begin_src R

  #+end_src

- Finally, unlist ~bar~, assign the result to ~baz~ and test that ~baz~ is a
  ~character~ vector:
  #+begin_src R

  #+end_src

- Exercise: can you do the three steps - ~removePunctuation~, ~strsplit~
  and ~unlist~ in one command starting with ~sentence~?
  #+begin_src R

  #+end_src

- Back to the main course: use ~stemDocument~ on ~baz~ and assign the
  result to ~stem_doc~:
  #+begin_src R

  #+end_src

- Re-complete the stemmed document with ~stemCompletion~ using ~comp_dict~
  as reference dictionary and save the result in ~complete_doc~:
  #+begin_src R

  #+end_src

- This is the expected result: ~complete_doc~ is a named ~character~
  vector whose names are the word stems (only ~complic~ was stemmed),
  and whose values are the completed words.
  #+begin_src R

  #+end_src

* Apply preprocessing steps to a corpus

- Reload the coffee corpus if you don't have it anymore in your R
  session - check this:
  #+name: check_coffee_data
  #+begin_src R
    any(ls()=="coffee_corpus")
    any(search()=="package:qdap")
    any(search()=="package:tm")
  #+end_src

- Reload it in case and load the necessary libraries, then run the
  search again:
  #+begin_src R
    <<load_coffee_data>>
    <<check_coffee_data>>
  #+end_src

- Example: remove the numbers from tweet no. 2 in ~coffee_corpus~ using
  ~tm_map~ and ~removeNumbers~:
  #+begin_src R
    corpus <- tm_map(coffee_corpus,removeNumbers)  # remove numbers
    content(coffee_corpus[[2]]) # original tweet
    content(corpus[[2]])  # tweet with numbers removed
  #+end_src

- Exercise: first edit the custom function ~clean_corpus()~ in the
  sample code to apply (in order):
  1) tm's ~removePunctuation()~.
  2) Base R's ~tolower()~.
  3) Remove the word "coffee" with ~tm::removeWords~
  4) Remove all white space with ~tm::stripWhitespace~
  #+begin_src R :results silent
    clean_corpus <- function(corpus) {
      ## removePunctuation from corpus with tm_map
      corpus <-
      ## transform corpus to lower with base::tolower and tm_map
      corpus <-
      ## removeWords "coffee" and stop words from corpus with tm_map
      corpus <-
      ## stripWhitespace from corpus with tm_map
      corpus <-
      ## return the cleaned corpus 
      return(corpus)   
    }
  #+end_src

- The function ~clean_corpus~ will now run all its content functions on
  any corpus argument - to test this:
  1) run ~clean_corpus~ on ~coffee_corpus~ and save it as ~clean_coffee~
  2) print the cleaned 227th tweet using ~[[~ and ~content~
  3) Compare it to the original tweet from ~coffee_corpus~.
  #+begin_src R 
    clean_corp <- clean_corpus(coffee_corpus)
    content(clean_corp[[999]]) # lower case, no punctuation, no "coffee"
    content(coffee_corpus[[999]])
  #+end_src

