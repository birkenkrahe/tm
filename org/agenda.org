#+TITLE: Agenda - Digital Humanities (CSC 105)
#+AUTHOR: Marcus Birkenkrahe
#+SUBTITLE: Lyon College, Spring 2023
#+STARTUP:overview hideblocks indent inlineimages
#+OPTIONS: toc:nil num:nil ^:nil
#+PROPERTY: header-args:R :session *R* :results output :exports both :noweb yes
* Week 1 - Course overview & text mining overview
#+attr_html: :width 400px
[[../img/cover.jpg]]

- [X] Course overview - assignments, grading, topics, platforms
- [X] Introduction to text mining with examples
- [ ] DataCamp assignment "Jumping into Text Mining with Bag-of-Words"
- [ ] Introduction to R programming for the assignment

* Week 2 - Text mining in practice & example
#+attr_html: :width 400px
[[../img/2_mess.jpg]]

- [X] *new* Reduced the no. of home assignments from 10 to 8 (more pts)
- [X] Text mining with Bag of Words explained
- [X] R environment information: console, packages and help
- [X] Setting up Emacs + ESS + R on your PC (Canvas announcement)
- [ ] Text mining with syntactic or semantic parsing explained
- [ ] Extended example: airline customer service tweets
- [ ] String manipulation functions in R

** Review week 1

- [X] What are topic/deliverables of your final project?[fn:1]
- [X] What are "structured data" in the context of text mining?[fn:2]
- [X] What does assessment bias refer to ("wisdom of crowds")[fn:3]

* Week 3 - Bag of words I - introduction
#+attr_latex: :width 400px
[[../img/bagofwords.png]]

- [X] Download the (raw) practice file again (changes) from GitHub
- [X] Finish the practice file and upload it to Canvas
- [X] Answer review questions
- [X] DataCamp lesson reduced
- [X] How to study for the test
** Review questions week 2
#+attr_latex: :width 400px
[[../img/review.png]]

Introduction to Bag-of-Words:
1) [X] What is a /corpus/ in the context of text mining?
2) [X] Which two text mining /techniques/ did we discuss?
3) [X] What is a "Document Term Matrix" (DTM)
4) [X] What is the difference between /syntactic/ and /semantic/?

Practice session:
1) [X] How can you see which R packages are currently loaded?
2) [X] How to get vector type, length and no. of characters?
3) [X] Which plot type is suitable to visualize a TDM?
4) [X] What is a nominal, what is an ordinal factor vector?

DataCamp lesson review:
1) [X] What is a /volatile corpus/?
2) [X] What are the steps to get a volatile corpus from a vector?
3) [X] What's the difference to getting a corpus from a dataframe?
4) [X] What data structure are source and corpus stored in?

** Test 1 on Thursday, February 9th
#+attr_latex: :width 400px
[[../img/mining.png]]

- 10-15 multiple choice questions, 15-20 minutes time
- test online in class between 2.30 pm and 2.50 pm
- followed by a quick test review
- the test will be opened for unlimited attempts afterwards

** [[https://lyon.instructure.com/courses/1015/assignments/5895][DataCamp lesson due next Thursday (2 Feb)]]
#+attr_latex: :width 400px
#+caption: Time for the next DataCamp assignment may be running out
[[../img/late.jpg]]

- You only have to complete the first third of the lesson
- Send me a screenshot for proof
- See [[https://lyon.instructure.com/courses/1015/discussion_topics/2088][Canvas announcement]]

** Condition for repeating tests
#+attr_latex: :width 400px
#+caption: Late or missed the test? Talk to me!
[[../img/late1.jpg]]

- If you inform me beforehand that you cannot attend an announced
  test, we can make arrangements for you to take the test outside of
  class.

** Finish first practice file and upload it

- Open your practice file with Emacs in ~Downloads~:
  #+begin_example sh
    emacs --file  1_bag_of_words_practice.org
  #+end_example

- Open Canvas, find the [[https://lyon.instructure.com/courses/1015/assignments/5896][assignment]] and upload the org file
  #+attr_latex: :width 400px
  [[../img/canvas.png]]

- Download the next practice file [[https://raw.githubusercontent.com/birkenkrahe/tm/main/org/3_corpus_practice.org][from GitHub]], save it as
  ~3_corpus_practice.org~ and open it in Emacs during the lecture.

** How should you study for data science tests?
#+attr_html: :width 400px
[[../img/studying.jpg]]

- If you were successful in the test: what did you do?
- If not: what do you think you should have done?

* Week 4 - Icestorm
#+attr_latex: :width 400px
[[../img/icestorm.jpg]]

* Week 5 - Bag of words II - corpus creation
#+attr_latex: :width 300px
[[../img/bagofwords.jpeg]]

- [X] Review DataCamp lesson ([[https://app.datacamp.com/groups/lyon-college-data-science-spring-2023/assignments/past-due][40% completed]]) - what went wrong?
- [X] Practice file upload to Canvas - ([[https://lyon.instructure.com/courses/1015/assignments/5896][40% completed]]) - why?
- [X] Test 1 end of the week - complete at your leisure by Tue-14-Feb
- [X] Next DataCamp lesson [[https://campus.datacamp.com/courses/text-mining-with-bag-of-words-in-r/jumping-into-text-mining-with-bag-of-words?ex=10]["Cleaning and preprocessing text"]] (Feb-16)
- [X] Back to [[https://github.com/birkenkrahe/tm/blob/main/org/3_corpus.org]["Bag-of-Words"]] (lecture + practice) - corpus creation
- [X] Install the ~tm~ package using R on your command line:
  #+begin_example
  c:/User/you> R
  R> install.packages(tm)
  #+end_example

/Tip:/ test questions will be sourced from the review questions ([[https://lyon.instructure.com/courses/1015/external_tools/173][zoom]])
#+attr_latex: :width 300px
[[../img/zoom.png]]

* Week 6 - Bag of words III - data cleaning
#+attr_latex: :width 400px
[[../img/4_cleaning.jpg]]

** Get bonus points when practicing

#+attr_latex: :width 200px
[[../img/datacamp2.png]]
- You can get 10 bonus points if you keep a practice streak of 10 days
- You can do this up to 3 times for a maximum of 30 points, which will
  be applied to your weakest final grade category
- Submit a screenshot of your mobile (or desktop) streak in Canvas
- If you lose your streak between day 5 and 10, you still get 5 points
- On the dashboard, DataCamp will suggest practice categories for you,
  and also in the mobile app
- This option ends on May 3rd (last day of spring term)
- You can get this bonus only in one of my courses (if you attend > 1)
#+attr_latex: :width 400p
[[../img/datacamp3.png]]

** GNU Treats: ~speed-type~, ~treemacs~ and ~gtypist~

- An attractive alternative to ~Dired~ is the ~treemacs~ package. It
  looks like this on my PC (and also works for the terminal Emacs):
  #+attr_latex: :width 400px
  [[../img/t_treemacs.png]]

- If you want to be faster on the keyboard, try [[https://www.gnu.org/savannah-checkouts/gnu/gtypist/gtypist.html#:~:text=GNU%20Typist%20(also%20called%20gtypist,the%20GNU%20General%20Public%20License.][GNU Typist]], a free
  10-lesson online trainer for increasing your typing skills.
  #+attr_latex: :width 400px
  [[../img/gtypist.png]]

- There is also an Emacs package to practice touch/speed typing in
  Emacs called ~speed-type~. You have to install it with ~M-x
  package-list-packages~, then find the package in the list and install
  with ~i~ and ~x~. [[https://github.com/dakra/speed-type][More information on GitHub.]]

b* Week 6 - Bag of words III - text cleaning
#+attr_latex: :width 400px
[[../img/4_cleaning.jpg]]

- Onward: Cleaning and preprocessing text (3rd part of DataCamp lesson 1)
- Part I: cleaning with ~tm~ and ~qdap~ functions
- Part II: using stopwords dictionaries
- Get ~4_cleaning_practice.org~ [[https://raw.githubusercontent.com/birkenkrahe/tm/main/org/4_cleaning_practice.org][from GitHub]] to code alongside me
- When finished, upload the file to Canvas

** DataCamp: all assignments & test 2 are live now
#+attr_latex: :width 400px
[[../img/canvas1.png]]

- Test 2 deadline: Friday, Feb 17 at 11:59 pm - open book!
- Don't leave the DataCamp assignments to the very end!

** Review questions for test 2

- Questions we have yet to review come from the last lecture:

  1) [X] counting characters, e.g. for "ChatGPT is an OpenAI bot"
     #+begin_src R
       nchar("Rhett knew the answer to this one")
     #+end_src

     #+RESULTS:
     : [1] 33

  2) [X] data in a ~factor~ vector - example: make a factor vector with
     the levels "up" and "down" and with elements
     ~{"up","up","down","up"}~ - is this ~factor~ ordered?
     #+begin_src R
       factor(
         c("up","up","down","up","down","down"),
         levels=c("down","up"),
         ordered=TRUE) -> fc
       fc
       is.ordered(fc)
     #+end_src

     #+RESULTS:
     : [1] up   up   down up   down down
     : Levels: down < up
     : [1] TRUE

  3) [X] Which functions show you the data of a dataframe,
     e.g. ~mtcars~?
     #+begin_src R
       mtcars |> str()
       mtcars |> head()
     #+end_src

     #+RESULTS:
     #+begin_example
     'data.frame':	32 obs. of  11 variables:
      $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...
      $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...
      $ disp: num  160 160 108 258 360 ...
      $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...
      $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...
      $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...
      $ qsec: num  16.5 17 18.6 19.4 17 ...
      $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...
      $ am  : num  1 1 1 0 0 0 0 0 0 0 ...
      $ gear: num  4 4 4 3 3 3 3 4 4 4 ...
      $ carb: num  4 4 1 1 2 1 4 2 2 4 ...
                        mpg cyl disp  hp drat    wt  qsec vs am gear carb
     Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4
     Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4
     Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1
     Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1
     Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2
     Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1
     #+end_example

  4) [X] Which functions show you the data of a text corpus?
     E.g. the 999th element of ~coffee_corpus~:
     #+begin_src R :results silent
       library(tm)
       coffee_df <- read.csv("~/Downloads/coffee.csv") # dataframe
       coffee_vec <- coffee_df$text # vector
       coffee_src <- VectorSource(coffee_vec) # source
       cc <- VCorpus(coffee_src)  # corpus
     #+end_src
     #+begin_src R
       ## with content()
       content(cc[[2]])
       ## with index operator [] 
       cc[[2]][1]
       ## with inspect()
       inspect(cc[[2]])
       ## NOT with meta()
       meta(cc[[2]])
     #+end_src

     #+RESULTS:
     #+begin_example
     [1] "RT @bryzy_brib: Senior March tmw morning at 7:25 A.M. in the SENIOR lot. Get up early, make yo coffee/breakfast, cus this will only happen ?"
     $content
     [1] "RT @bryzy_brib: Senior March tmw morning at 7:25 A.M. in the SENIOR lot. Get up early, make yo coffee/breakfast, cus this will only happen ?"
     <<PlainTextDocument>>
     Metadata:  7
     Content:  chars: 140

     RT @bryzy_brib: Senior March tmw morning at 7:25 A.M. in the SENIOR lot. Get up early, make yo coffee/breakfast, cus this will only happen ?
       author       : character(0)
       datetimestamp: 2023-02-14 21:44:47
       description  : character(0)
       heading      : character(0)
       id           : 2
       language     : en
       origin       : character(0)
     #+end_example

  5) [X] Extract the 999th tweet of the list ~LIST~ (stored in the list
     element ~tweets~:
     #+begin_src R :results silent
       LIST <- list(letters=LETTERS[1:4],
                    numbers=1:100,
                    tweets=coffee_vec)
     #+end_src     
     #+begin_src R
       LIST[["tweets"]][999]
      #+end_src

      #+RESULTS:
      : [1] "First morning coffee after Ramadan http://t.co/ZEu6cl9qGY"


** NEXT [[https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/][What is ChatGPT Doing and Why Does It Work?]]

* Week 7 - Bag of words IV - stop words
#+attr_latex: :width 400px
[[../img/tdm.jpg]]

- *REMEMBER TO FINISH THE FIRST DATACAMP LESSON BY FRIDAY*
  
- Stopwords - using ~stopwords~ and adding stop words
- Finding strings in a big data set
- Finding tweets in the coffee tweet vector
- Removing words from the stop words dictionary

* Week 8 - Bag of words V - clean corpus
#+attr_latex: :width 400px
[[../img/lotr.jpg]]

Finish stopwords & TDM/DTM (DataCamp chapter 1)
- Stem completion on a sentence 
- Splitting strings with ~strsplit~
- Applying preprocessing steps to a corpus
- TDM and DTM

** Review: week 7
#+attr_latex: :width 400px
[[../img/LOTR-header.jpg]]

1) [X] What is the process from CSV file to document corpus?
   #+begin_notes
   1) ~read.csv~ into data frame ~df~
   2) extract text vector from data frame ~df~ as ~df$text~
   3) turn text vector into a source with ~VectorSource~
   4) turn source into volatile (in-memory) corpus with ~VCorpus~
   #+end_notes

2) [X] How do you load the package ~tm~ and check if it contains a
   function named "stopwords"?
   #+begin_src R
     library(tm)
     search()
     any(ls('package:tm')=="stopwords")
   #+end_src

   #+RESULTS:
   : Loading required package: NLP
   :  [1] ".GlobalEnv"        "package:tm"        "package:NLP"      
   :  [4] "package:class"     "ESSR"              "package:stats"    
   :  [7] "package:graphics"  "package:grDevices" "package:utils"    
   : [10] "package:datasets"  "package:methods"   "Autoloads"        
   : [13] "package:base"
   : [1] TRUE

3) [X] What data structure is ~stopwords("en")~, and how many words does
   it contain?
   #+begin_src R
     str(stopwords("en"))  ## a character vector
     length(stopwords("en"))
   #+end_src

   #+RESULTS:
   :  chr [1:174] "i" "me" "my" "myself" "we" "our" "ours" "ourselves" "you" ...
   : [1] 174

4) [X] How can you find out if a particular word, say "cannot", is
   contained in the stopwords English dictionary?
   #+begin_src R
     any(stopwords("en")=="cannot")
   #+end_src

   #+RESULTS:
   : [1] TRUE

5) [X] How would you add two new words like "coffee" and "bean" to the
   END of the stopwords English dictionary, and then check for them?
   #+begin_src R
     c(stopwords("en"),"coffee","bean") |> tail() ## pipeline
     tail(c(stopwords("en"),"coffee","bean"))  ## nested
     tail(stopwords("en"))
   #+end_src

   #+RESULTS:
   : [1] "so"     "than"   "too"    "very"   "coffee" "bean"
   : [1] "so"     "than"   "too"    "very"   "coffee" "bean"
   : [1] "own"  "same" "so"   "than" "too"  "very"

6) [X] Why does ~which(c(20,30,40)==30)~ return the value ~2~ ?
   #+begin_src R
     c("a","b","c") == "b"  ## this is FALSE TRUE FALSE
     which( c("a","b","c") == "b")  ## returns index for TRUE
   #+end_src

   #+RESULTS:
   : [1] FALSE  TRUE FALSE
   : [1] 2

7) [ ] Which index of the stopwords English dictionary corresponds to
   the word "with"?
   #+begin_src R
     which(stopwords("en") == "with")
     stopwords("en")[124]
   #+end_src

   #+RESULTS:
   : [1] 124
   : [1] "with"

8) [ ] Use ~grepl(pattern,x)~ to find out if any words beginning with
   "no" are in the SPANISH stopwords dictionary ~x=stopwords("es")~ -
   Tip: the pattern for "word beginning with 'no'" is "^no"
   #+begin_src R
      no <- grepl(pattern="^no", x=stopwords("es"))  # store pattern
      any(no) # check if any elements of of 'no' are TRUE
      which(no)  # which elements are these?
      stopwords("es")[which(no)]  # which words start with 'no'?

     ## the whole command as a series of nested functions:
     stopwords("es")[which(grepl(pattern="^no",x=stopwords("es")))]
     #+end_src

   #+RESULTS:
   : [1] TRUE
   : [1] 16 45 83 92
   : [1] "no"       "nos"      "nosotros" "nosotras"
   : [1] "no"       "nos"      "nosotros" "nosotras"

9) [ ] Do any words in ~stopwords("es")~ end with ~de~? Which ones? (The
   pattern for words ending in "de" is ~*de~).
   #+begin_src R
     es <- stopwords("es")   ## store spanish stopwords
     de_in_es <- grepl("*de",es)  ## store logical pattern vector
     any(de_in_es)  ## check if any words ending with 'de' are there
     idx <- which(de_in_es) # store their indices in idx
     es[idx]  # print the words indexed with idx
   #+end_src

   #+RESULTS:
   : [1] TRUE
   : [1] "de"    "del"   "donde" "desde"

+* TODO Week 9: Word clouds
#+attr_latex: :width 400px
[[../img/clouds.png]]

- Finish TDM and DTM
- Fun with ChatGPT in R
  
- *Word clouds and more interesting visuals:*
  1) Frequent terms with ~tm~ and ~qdap~
  2) Word loud visualizations
  3) Common word visualization
  4) Word clustering

* Week 9 - Visualization I
#+attr_latex: :width 400px
#+caption: Michelangelo, Creation of Adam (1512)
[[../img/7_michelangelo.png]]

** Fun with R: New ChatGPT API - use bot in R
#+attr_latex: :width 400px
[[../img/chatgpt.jpg]]

OpenAI has opened the ChatGPT API, and we can access it from within R,
courtesy of Rasmus Bååth (2023):
1) [[https://chat.openai.com/auth/loginLinks][Register with OpenAI here]] (use your Lyon GMail account)
2) Get your secret ~api_key~ [[https://platform.openai.com/account/api-keys][here from OpenAI]] (you need to register)
3) [[https://gist.githubusercontent.com/rasmusab/c25badf55f5dacee14ab13834798d3ef/raw/e3af90fc32d91c974cd2ec9ddb2f7bf52e992cff/chat-gtp-api-call.R][Add this code]] to your ~~/.Rprofile~ file (including the secret key)
4) Install ~httr~ and ~stringr~
5) Now the function ~ask_chatgpt~ will be available to you (GitHub typo)

[[https://www.sumsar.net/blog/call-chatgpt-from-r/][From this post]], you can also learn how to clean the messages from
ChatGPT using standard text mining functions.

** Load CSV data, create and clean corpora:
#+attr_latex: :width 400px
[[../img/7_tweets.png]]

- Download and run ~corpora.org~ (read the README): [[https://bit.ly/corpora_org][bit.ly/corpora_org]]

** Look at next DataCamp session
#+attr_latex: :width 400px
[[../img/datacamp4.png]]

- Mostly straightforward: video shows the code that you then work out
  in the editor/console.

- If you have time, check the help pages for different functions once
  you've loaded the packages.

* Week 10: Visualization II
#+attr_latex: :width 400px
[[../img/piday.png]]

- *NO CLASS MEETING ON THURSDAY*: work on your final project instead!
- Read Chomsky on ChatGPT and extract discussion points/insights
- We'll briefly discuss this article after spring break
  
** READ Spring break reading: Chomsky on ChatGPT
#+attr_latex: :width 400px
#+caption: Noam Chomsky, MIT.
[[../img/chomsky.png]]

 - Read "[[https://www-proquest-com.lyon.idm.oclc.org/docview/2784188893?accountid=12202&parentSessionId=iErhLXpJwsbfjL%2BhcvJIBN0A8ZPM%2FnW3d7hP0RaJpG0%3D][The False Promise of ChatGPT]]" to discuss after spring break
 - Print copy distributed to you for easier access
 - Collect the main points of the article and your questions

* Week 11: Visualization III - wordclouds
#+attr_latex: :width 400px
[[../img/clouds.jpg]]

*Reading review: "The False Promise of ChatGPT"*
- What's your take on the article?
- Contrast with [[https://garymarcus.substack.com/p/caricaturing-noam-chomsky][Gary Marcus's warnings]] / [[https://www.gatesnotes.com/The-Age-of-AI-Has-Begun][Bill Gates' gushing]]

*DataCamp assignments: Redux*
- Reduced the number of assignments by 50% (points go up)
- Make sure that you complete the exercises (don't phone it in)

*Test 3 live from today until next Tuesday:*
- Covers DTM/TDM and visualization with ~tm~, ~qdap~.
- If you miss the deadline, you still get partial credit (60%)
  
*This week:*
- Intro to word clouds and word networks
- Finding common words and visualizing networks

** Test preview:

1) The Document-Term-Matrix (and the Term-Document-Matrix) is
   generated from a document /corpus/.
   #+begin_notes
   Answer: TRUE - remember that we did this only after cleaning a
   corpus
   #+end_notes
2) You're analyzing a dataset of SMS messages. When should you use a
   Term-Document-Matrix instead of a Document-Term-Matrix?
   #+begin_notes
   Answer: When you want the words as rows and the messages as columns
   #+end_notes
3) ~M~ is a 4 x 4 matrix. Are the following commands equivalent?
   #+begin_src R
     M <- matrix(1:16,4,4)
     M
     M[1:4,1:4]
     head(M,4)
   #+end_src

   #+RESULTS:
   #+begin_example
        [,1] [,2] [,3] [,4]
   [1,]    1    5    9   13
   [2,]    2    6   10   14
   [3,]    3    7   11   15
   [4,]    4    8   12   16
        [,1] [,2] [,3] [,4]
   [1,]    1    5    9   13
   [2,]    2    6   10   14
   [3,]    3    7   11   15
   [4,]    4    8   12   16
        [,1] [,2] [,3] [,4]
   [1,]    1    5    9   13
   [2,]    2    6   10   14
   [3,]    3    7   11   15
   [4,]    4    8   12   16
   #+end_example
4) What is the purpose of computing frequencies of words?
   #+begin_notes
   Answer: To count the words, to see how often they occur because the
   count may be correlated to meaning (e.g. popularity)
   #+end_notes
5) ~M~ is a large matrix. What is the output of the following
   command?
   #+begin_src R
     M <- matrix()
     rowSums(M) |> is.vector() |> print()
   #+end_src

   #+RESULTS:
   : [1] TRUE

   #+begin_notes
   Answer: A logical value (TRUE or FALSE)
   #+end_notes
6) How can you sum over the columns of the matrix ~M~? How many elements
   will the resulting vector have?
   #+begin_src R
    M <- matrix(data=1:6, nrow=2, byrow=TRUE)
    M
    colSums(M)
   #+end_src

   #+RESULTS:
   :      [,1] [,2] [,3]
   : [1,]    1    2    3
   : [2,]    4    5    6
   : [1] 5 7 9

7) ~foo~ is a frequency vector (it contains words and their
   counts). What does this command do?
   #+begin_example R
     head(sort(foo, decreasing = TRUE),n=3)
   #+end_example
   #+begin_src R
     foo <- c("we"=75,"me"=100,"you"=30)
     foo
     head(sort(foo, decreasing = TRUE))     
   #+end_src
8) A barplot requires numeric (not categorical) values as input.
   #+begin_notes
   Answer: FALSE. A histogram requires numeric input. A barplot
   requires categorical input - its only mandatory argument is the
   ~height~ of the bars.
   #+end_notes
9) Which command returns all functions in the package ~qdap~?
   #+begin_notes
     ls('package:qdap')
   #+end_notes
10) How can you display the data structure of a complicated frequency
    term vector like this one:
    #+begin_example R
    f1 <- freq_terms(text.var=coffee_df,
                     top = 10,
                     at.least = 3,
                     stopwords = "Top200Words")
    ## display structure of vector
    ...
    #+end_example
    #+begin_example org
      : 'data.frame':	10 obs. of  2 variables:
  :  $ WORD: chr  "false" "coffee" "for" "relnofollowtwitter" ...
  :  $ FREQ: num  2997 1004 781 600 381 ...
    #+end_example
    1) How can you plot a word frequency vector ~foo~:
       #+begin_example R
         > foo <- c("we"=75,"me"=100,"you"=30)
         > foo

         Output:
         :  we  me you 
         :  75 100  30
       #+end_example
       #+begin_src R :results graphics file :file ../img/fooplot.png
         bar <- sort(foo,decreasing=TRUE)
         par(mfrow=c(1,2),pty='s')
         barplot(bar, horiz=TRUE,las=1, main="barplot")
         plot(bar,main="scatterplot")
       #+end_src

       #+RESULTS:
       [[file:../img/fooplot.png]]

** Getting the corpus data (again!)
#+attr_latex: :width 300px
#+caption: J.D. Estes at Naval Air Base, Corpus Christi TX (Source: Flickr.com/LOC)
[[../img/torpedoes.jpg]]

- Download ~corpora.R~ from GitHub ([[https://bit.ly/tm-corpora][bit.ly/tm-corpora]])

- Run the file on the shell (~M-x eshell~) as a batch job:
  #+begin_src sh
    R CMD BATCH corpora.R
    ls -al .RData
  #+end_src

- Load the ~.RData~ file in your current R session and check that
  packages and user-defined objects were loaded:
  #+begin_src R
    load(".RData")
    search()
    ls()
  #+end_src
- If we don't finish with a session, save your data from now on:
  #+begin_src R
    save.image(file=".RData")
    shell("ls -al .RData")
  #+end_src

- You also need to load ~tm~, ~qdap~, ~SnowballC~ and ~wordcloud~. I put all
  of these in a function at the end of ~corpora.R~:
  #+name: load_packages
  #+begin_src R
    load_packages <- function() {
      library(tm)
      library(qdap)
      library(SnowballC)
      library(wordcloud)
      search()
    }
    load_packages()
  #+end_src

  #+RESULTS: load_packages
  #+begin_example
   [1] ".GlobalEnv"               "package:wordcloud"       
   [3] "package:SnowballC"        "package:qdap"            
   [5] "package:RColorBrewer"     "package:qdapTools"       
   [7] "package:qdapRegex"        "package:qdapDictionaries"
   [9] "package:tm"               "package:NLP"             
  [11] "ESSR"                     "package:stats"           
  [13] "package:graphics"         "package:grDevices"       
  [15] "package:utils"            "package:datasets"        
  [17] "package:stringr"          "package:httr"            
  [19] "package:methods"          "Autoloads"               
  [21] "package:base"
  #+end_example

** Bonus points for job fair experience report!
#+attr_latex: :width 400px
[[../img/fair_flickr_color.jpg]]

- Write long paragraph about your job fair experience for 10 points.
- Great opportunity to network, mix and mingle, and show off.
- Bring 1 page resume, a few questions, a story, and dress up.
- Must go: graduating seniors. Should go: everyone else.
- Motivate each other by going as a pair, a group, a team.
- Post your experience report [[https://lyon.instructure.com/courses/1015/assignments/9613][here in Canvas]].

** The Plan! Yes, we've got one!
#+attr_latex: :width 400px
#+caption: State Library of NSW - A.W.A. exhibition Sydney 1940
 [[../img/plan.jpg]]

- You'll plough through the remainder of "bag of words" on your own
- I'll cover "sentiment analysis" in lectures and practice:
  1. Visualize polarity in messages with ~qdap~
  2. Subjectivity lexicons with ~tidytext~
  3. Visualize sentiments with ~ggplot2~
  4. Case study: AirBnB review analysis

** Wordclouds continued
#+attr_latex: :width 400px
[[../img/berlinclouds.jpg]]

- Download ~8_wordclouds_practice.org~ again (I changed it)
- Open the file in Emacs and start a shell with ~M-x R~
- Download ~corpora.R~ again (I changed it)
- Run ~corpora.R~ as batch job on the shell
- Load ~Downloads/.RData~ 

** What did you learn this week?

Tue:
1. How to save and load a complete interactive R session
2. Run an R file in batch mode (R CMD BATCH)
3. To make a wordcloud you need to sum over the rows of a TDM

Thu:
1. How to use the ~wordcloud~ function
2. How to improve word clouds with different ~colors~
3. How to use prebuilt color palettes like ~viridisLite~
   
* Week 12: Pyramid plots and wordnets
#+attr_latex: :width 400px
#+caption: Source: Drag net fishing (Flickr.com)
[[../img/wordnet.jpg]]

- Useful: feed [[https://lyon.instructure.com/calendar][Canvas calendar]] to your Google calendar ([[https://community.canvaslms.com/t5/Student-Guide/How-do-I-subscribe-to-the-Calendar-feed-using-Google-Calendar-as/ta-p/535][instructions]])
- Look at [[https://elanmarkowitz.github.io/USC-CS662/][this advanced NLP course]] (USC) - you recognize our topics!

** What did you learn this week?

Tue:
1. How to visualize common and dissimilar words with ~wordcloud~
2. How to visualize common words from two datasets with ~plotrix~
3. How to visualize word networks with ~qdap~

Thu: NO CLASS - CAPSTONE AND TEST PREPARATION
  
* NEXT Week 13: Polarity scoring
#+attr_latex: :width 400px
#+caption: Polarity scoring of a conversation
[[../img/10_conversation.png]]

- Graded capstone project (unless you prefer to write an essay): 

* TODO Week 14: Visualizing sentiment
** Review: Dendrograms and N-gram tokenization

* TODO Week 15: Case study: Airbnb reviews
* References

- Rasmus Bååth (3 March 2023). Call ChatGPT (or really any other API)
  from R. URL: [[https://www.sumsar.net/blog/call-chatgpt-from-r/][www.sumsar.net]]

- Noam Chomsky, Ian Roberts and Jeffrey Watumull (9 March 2023). The
  False Promise of ChatGPT. URL: nytimes.com.

- Photo: J.D. Estes at the Naval Air Baise, Corpus Christi, TX
  (1939). Online: [[https://flic.kr/p/4jz6Qo][Flickr.com Library of Congress]].
  
* Footnotes

[fn:3] No /assessment bias:/ independent judgements, diverse
understanding, local knowledge and tabulated results.

[fn:2] Structured data is data in rectangular structures (e.g. tables)
whose rows are records or observations, and whose columns are feature
vectors or attributes. A vector of length N can also be printed as a
rectangular structure of dimension N x 1 (N rows, 1 column):
#+begin_src R
  vec <- c("jim", "joe", "jan", "bud")
  vec
  matrix(vec)
  matrix(vec,nrow=1)
#+end_src

#+RESULTS:
: [1] "jim" "joe" "jan" "bud"
:      [,1]
: [1,] "jim"
: [2,] "joe"
: [3,] "jan"
: [4,] "bud"
:      [,1]  [,2]  [,3]  [,4]
: [1,] "jim" "joe" "jan" "bud"

[fn:1] Proposal, literature review, presentation or essay, IMRaD
(Introduction, Method, Results and Discussion) structure.
