#+TITLE: Agenda - Digital Humanities (CSC 105)
#+AUTHOR: Marcus Birkenkrahe
#+SUBTITLE: Lyon College, Spring 2023
#+STARTUP:overview hideblocks indent inlineimages
#+OPTIONS: toc:nil num:nil ^:nil
#+PROPERTY: header-args:R :session *R* :results output :exports both :noweb yes
* Week 1 - Course overview & text mining overview
#+attr_html: :width 400px
[[../img/cover.jpg]]

- [X] Course overview - assignments, grading, topics, platforms
- [X] Introduction to text mining with examples
- [ ] DataCamp assignment "Jumping into Text Mining with Bag-of-Words"
- [ ] Introduction to R programming for the assignment

* Week 2 - Text mining in practice & example
#+attr_html: :width 400px
[[../img/2_mess.jpg]]

- [X] *new* Reduced the no. of home assignments from 10 to 8 (more pts)
- [X] Text mining with Bag of Words explained
- [X] R environment information: console, packages and help
- [X] Setting up Emacs + ESS + R on your PC (Canvas announcement)
- [ ] Text mining with syntactic or semantic parsing explained
- [ ] Extended example: airline customer service tweets
- [ ] String manipulation functions in R

** Review week 1

- [X] What are topic/deliverables of your final project?[fn:1]
- [X] What are "structured data" in the context of text mining?[fn:2]
- [X] What does assessment bias refer to ("wisdom of crowds")[fn:3]

* Week 3 - Bag of words I - introduction
#+attr_latex: :width 400px
[[../img/bagofwords.png]]

- [X] Download the (raw) practice file again (changes) from GitHub
- [X] Finish the practice file and upload it to Canvas
- [X] Answer review questions
- [X] DataCamp lesson reduced
- [X] How to study for the test
** Review questions week 2
#+attr_latex: :width 400px
[[../img/review.png]]

Introduction to Bag-of-Words:
1) [X] What is a /corpus/ in the context of text mining?
2) [X] Which two text mining /techniques/ did we discuss?
3) [X] What is a "Document Term Matrix" (DTM)
4) [X] What is the difference between /syntactic/ and /semantic/?

Practice session:
1) [X] How can you see which R packages are currently loaded?
2) [X] How to get vector type, length and no. of characters?
3) [X] Which plot type is suitable to visualize a TDM?
4) [X] What is a nominal, what is an ordinal factor vector?

DataCamp lesson review:
1) [X] What is a /volatile corpus/?
2) [X] What are the steps to get a volatile corpus from a vector?
3) [X] What's the difference to getting a corpus from a dataframe?
4) [X] What data structure are source and corpus stored in?

** Test 1 on Thursday, February 9th
#+attr_latex: :width 400px
[[../img/mining.png]]

- 10-15 multiple choice questions, 15-20 minutes time
- test online in class between 2.30 pm and 2.50 pm
- followed by a quick test review
- the test will be opened for unlimited attempts afterwards

** [[https://lyon.instructure.com/courses/1015/assignments/5895][DataCamp lesson due next Thursday (2 Feb)]]
#+attr_latex: :width 400px
#+caption: Time for the next DataCamp assignment may be running out
[[../img/late.jpg]]

- You only have to complete the first third of the lesson
- Send me a screenshot for proof
- See [[https://lyon.instructure.com/courses/1015/discussion_topics/2088][Canvas announcement]]

** Condition for repeating tests
#+attr_latex: :width 400px
#+caption: Late or missed the test? Talk to me!
[[../img/late1.jpg]]

- If you inform me beforehand that you cannot attend an announced
  test, we can make arrangements for you to take the test outside of
  class.

** Finish first practice file and upload it

- Open your practice file with Emacs in ~Downloads~:
  #+begin_example sh
    emacs --file  1_bag_of_words_practice.org
  #+end_example

- Open Canvas, find the [[https://lyon.instructure.com/courses/1015/assignments/5896][assignment]] and upload the org file
  #+attr_latex: :width 400px
  [[../img/canvas.png]]

- Download the next practice file [[https://raw.githubusercontent.com/birkenkrahe/tm/main/org/3_corpus_practice.org][from GitHub]], save it as
  ~3_corpus_practice.org~ and open it in Emacs during the lecture.

** How should you study for data science tests?
#+attr_html: :width 400px
[[../img/studying.jpg]]

- If you were successful in the test: what did you do?
- If not: what do you think you should have done?

* Week 4 - Icestorm
#+attr_latex: :width 400px
[[../img/icestorm.jpg]]

* Week 5 - Bag of words II - corpus creation
#+attr_latex: :width 300px
[[../img/bagofwords.jpeg]]

- [X] Review DataCamp lesson ([[https://app.datacamp.com/groups/lyon-college-data-science-spring-2023/assignments/past-due][40% completed]]) - what went wrong?
- [X] Practice file upload to Canvas - ([[https://lyon.instructure.com/courses/1015/assignments/5896][40% completed]]) - why?
- [X] Test 1 end of the week - complete at your leisure by Tue-14-Feb
- [X] Next DataCamp lesson [[https://campus.datacamp.com/courses/text-mining-with-bag-of-words-in-r/jumping-into-text-mining-with-bag-of-words?ex=10]["Cleaning and preprocessing text"]] (Feb-16)
- [X] Back to [[https://github.com/birkenkrahe/tm/blob/main/org/3_corpus.org]["Bag-of-Words"]] (lecture + practice) - corpus creation
- [X] Install the ~tm~ package using R on your command line:
  #+begin_example
  c:/User/you> R
  R> install.packages(tm)
  #+end_example

/Tip:/ test questions will be sourced from the review questions ([[https://lyon.instructure.com/courses/1015/external_tools/173][zoom]])
#+attr_latex: :width 300px
[[../img/zoom.png]]

* Week 6 - Bag of words III - data cleaning
#+attr_latex: :width 400px
[[../img/4_cleaning.jpg]]

** Get bonus points when practicing

#+attr_latex: :width 200px
[[../img/datacamp2.png]]
- You can get 10 bonus points if you keep a practice streak of 10 days
- You can do this up to 3 times for a maximum of 30 points, which will
  be applied to your weakest final grade category
- Submit a screenshot of your mobile (or desktop) streak in Canvas
- If you lose your streak between day 5 and 10, you still get 5 points
- On the dashboard, DataCamp will suggest practice categories for you,
  and also in the mobile app
- This option ends on May 3rd (last day of spring term)
- You can get this bonus only in one of my courses (if you attend > 1)
#+attr_latex: :width 400p
[[../img/datacamp3.png]]

** GNU Treats: ~speed-type~, ~treemacs~ and ~gtypist~

- An attractive alternative to ~Dired~ is the ~treemacs~ package. It
  looks like this on my PC (and also works for the terminal Emacs):
  #+attr_latex: :width 400px
  [[../img/t_treemacs.png]]

- If you want to be faster on the keyboard, try [[https://www.gnu.org/savannah-checkouts/gnu/gtypist/gtypist.html#:~:text=GNU%20Typist%20(also%20called%20gtypist,the%20GNU%20General%20Public%20License.][GNU Typist]], a free
  10-lesson online trainer for increasing your typing skills.
  #+attr_latex: :width 400px
  [[../img/gtypist.png]]

- There is also an Emacs package to practice touch/speed typing in
  Emacs called ~speed-type~. You have to install it with ~M-x
  package-list-packages~, then find the package in the list and install
  with ~i~ and ~x~. [[https://github.com/dakra/speed-type][More information on GitHub.]]

b* Week 6 - Bag of words III - text cleaning
#+attr_latex: :width 400px
[[../img/4_cleaning.jpg]]

- Onward: Cleaning and preprocessing text (3rd part of DataCamp lesson 1)
- Part I: cleaning with ~tm~ and ~qdap~ functions
- Part II: using stopwords dictionaries
- Get ~4_cleaning_practice.org~ [[https://raw.githubusercontent.com/birkenkrahe/tm/main/org/4_cleaning_practice.org][from GitHub]] to code alongside me
- When finished, upload the file to Canvas

** DataCamp: all assignments & test 2 are live now
#+attr_latex: :width 400px
[[../img/canvas1.png]]

- Test 2 deadline: Friday, Feb 17 at 11:59 pm - open book!
- Don't leave the DataCamp assignments to the very end!

** Review questions for test 2

- Questions we have yet to review come from the last lecture:

  1) [X] counting characters, e.g. for "ChatGPT is an OpenAI bot"
     #+begin_src R
       nchar("Rhett knew the answer to this one")
     #+end_src

     #+RESULTS:
     : [1] 33

  2) [X] data in a ~factor~ vector - example: make a factor vector with
     the levels "up" and "down" and with elements
     ~{"up","up","down","up"}~ - is this ~factor~ ordered?
     #+begin_src R
       factor(
         c("up","up","down","up","down","down"),
         levels=c("down","up"),
         ordered=TRUE) -> fc
       fc
       is.ordered(fc)
     #+end_src

     #+RESULTS:
     : [1] up   up   down up   down down
     : Levels: down < up
     : [1] TRUE

  3) [X] Which functions show you the data of a dataframe,
     e.g. ~mtcars~?
     #+begin_src R
       mtcars |> str()
       mtcars |> head()
     #+end_src

     #+RESULTS:
     #+begin_example
     'data.frame':	32 obs. of  11 variables:
      $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...
      $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...
      $ disp: num  160 160 108 258 360 ...
      $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...
      $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...
      $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...
      $ qsec: num  16.5 17 18.6 19.4 17 ...
      $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...
      $ am  : num  1 1 1 0 0 0 0 0 0 0 ...
      $ gear: num  4 4 4 3 3 3 3 4 4 4 ...
      $ carb: num  4 4 1 1 2 1 4 2 2 4 ...
                        mpg cyl disp  hp drat    wt  qsec vs am gear carb
     Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4
     Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4
     Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1
     Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1
     Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2
     Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1
     #+end_example

  4) [X] Which functions show you the data of a text corpus?
     E.g. the 999th element of ~coffee_corpus~:
     #+begin_src R :results silent
       library(tm)
       coffee_df <- read.csv("~/Downloads/coffee.csv") # dataframe
       coffee_vec <- coffee_df$text # vector
       coffee_src <- VectorSource(coffee_vec) # source
       cc <- VCorpus(coffee_src)  # corpus
     #+end_src
     #+begin_src R
       ## with content()
       content(cc[[2]])
       ## with index operator [] 
       cc[[2]][1]
       ## with inspect()
       inspect(cc[[2]])
       ## NOT with meta()
       meta(cc[[2]])
     #+end_src

     #+RESULTS:
     #+begin_example
     [1] "RT @bryzy_brib: Senior March tmw morning at 7:25 A.M. in the SENIOR lot. Get up early, make yo coffee/breakfast, cus this will only happen ?"
     $content
     [1] "RT @bryzy_brib: Senior March tmw morning at 7:25 A.M. in the SENIOR lot. Get up early, make yo coffee/breakfast, cus this will only happen ?"
     <<PlainTextDocument>>
     Metadata:  7
     Content:  chars: 140

     RT @bryzy_brib: Senior March tmw morning at 7:25 A.M. in the SENIOR lot. Get up early, make yo coffee/breakfast, cus this will only happen ?
       author       : character(0)
       datetimestamp: 2023-02-14 21:44:47
       description  : character(0)
       heading      : character(0)
       id           : 2
       language     : en
       origin       : character(0)
     #+end_example

  5) [X] Extract the 999th tweet of the list ~LIST~ (stored in the list
     element ~tweets~:
     #+begin_src R :results silent
       LIST <- list(letters=LETTERS[1:4],
                    numbers=1:100,
                    tweets=coffee_vec)
     #+end_src     
     #+begin_src R
       LIST[["tweets"]][999]
      #+end_src

      #+RESULTS:
      : [1] "First morning coffee after Ramadan http://t.co/ZEu6cl9qGY"


** NEXT [[https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/][What is ChatGPT Doing and Why Does It Work?]]

* Week 7 - Bag of words IV - stop words
#+attr_latex: :width 400px
[[../img/tdm.jpg]]

- *REMEMBER TO FINISH THE FIRST DATACAMP LESSON BY FRIDAY*
  
- Stopwords - using ~stopwords~ and adding stop words
- Finding strings in a big data set
- Finding tweets in the coffee tweet vector
- Removing words from the stop words dictionary

* Week 8 - Bag of words V - clean corpus
#+attr_latex: :width 400px
[[../img/lotr.jpg]]

Finish stopwords & TDM/DTM (DataCamp chapter 1)
- Stem completion on a sentence 
- Splitting strings with ~strsplit~
- Applying preprocessing steps to a corpus
- TDM and DTM

** Review: week 7
#+attr_latex: :width 400px
[[../img/LOTR-header.jpg]]

1) [X] What is the process from CSV file to document corpus?
   #+begin_notes
   1) ~read.csv~ into data frame ~df~
   2) extract text vector from data frame ~df~ as ~df$text~
   3) turn text vector into a source with ~VectorSource~
   4) turn source into volatile (in-memory) corpus with ~VCorpus~
   #+end_notes

2) [X] How do you load the package ~tm~ and check if it contains a
   function named "stopwords"?
   #+begin_src R
     library(tm)
     search()
     any(ls('package:tm')=="stopwords")
   #+end_src

   #+RESULTS:
   : Loading required package: NLP
   :  [1] ".GlobalEnv"        "package:tm"        "package:NLP"      
   :  [4] "package:class"     "ESSR"              "package:stats"    
   :  [7] "package:graphics"  "package:grDevices" "package:utils"    
   : [10] "package:datasets"  "package:methods"   "Autoloads"        
   : [13] "package:base"
   : [1] TRUE

3) [X] What data structure is ~stopwords("en")~, and how many words does
   it contain?
   #+begin_src R
     str(stopwords("en"))  ## a character vector
     length(stopwords("en"))
   #+end_src

   #+RESULTS:
   :  chr [1:174] "i" "me" "my" "myself" "we" "our" "ours" "ourselves" "you" ...
   : [1] 174

4) [X] How can you find out if a particular word, say "cannot", is
   contained in the stopwords English dictionary?
   #+begin_src R
     any(stopwords("en")=="cannot")
   #+end_src

   #+RESULTS:
   : [1] TRUE

5) [X] How would you add two new words like "coffee" and "bean" to the
   END of the stopwords English dictionary, and then check for them?
   #+begin_src R
     c(stopwords("en"),"coffee","bean") |> tail() ## pipeline
     tail(c(stopwords("en"),"coffee","bean"))  ## nested
     tail(stopwords("en"))
   #+end_src

   #+RESULTS:
   : [1] "so"     "than"   "too"    "very"   "coffee" "bean"
   : [1] "so"     "than"   "too"    "very"   "coffee" "bean"
   : [1] "own"  "same" "so"   "than" "too"  "very"

6) [X] Why does ~which(c(20,30,40)==30)~ return the value ~2~ ?
   #+begin_src R
     c("a","b","c") == "b"  ## this is FALSE TRUE FALSE
     which( c("a","b","c") == "b")  ## returns index for TRUE
   #+end_src

   #+RESULTS:
   : [1] FALSE  TRUE FALSE
   : [1] 2

7) [ ] Which index of the stopwords English dictionary corresponds to
   the word "with"?
   #+begin_src R
     which(stopwords("en") == "with")
     stopwords("en")[124]
   #+end_src

   #+RESULTS:
   : [1] 124
   : [1] "with"

8) [ ] Use ~grepl(pattern,x)~ to find out if any words beginning with
   "no" are in the SPANISH stopwords dictionary ~x=stopwords("es")~ -
   Tip: the pattern for "word beginning with 'no'" is "^no"
   #+begin_src R
      no <- grepl(pattern="^no", x=stopwords("es"))  # store pattern
      any(no) # check if any elements of of 'no' are TRUE
      which(no)  # which elements are these?
      stopwords("es")[which(no)]  # which words start with 'no'?

     ## the whole command as a series of nested functions:
     stopwords("es")[which(grepl(pattern="^no",x=stopwords("es")))]
     #+end_src

   #+RESULTS:
   : [1] TRUE
   : [1] 16 45 83 92
   : [1] "no"       "nos"      "nosotros" "nosotras"
   : [1] "no"       "nos"      "nosotros" "nosotras"

9) [ ] Do any words in ~stopwords("es")~ end with ~de~? Which ones? (The
   pattern for words ending in "de" is ~*de~).
   #+begin_src R
     es <- stopwords("es")   ## store spanish stopwords
     de_in_es <- grepl("*de",es)  ## store logical pattern vector
     any(de_in_es)  ## check if any words ending with 'de' are there
     idx <- which(de_in_es) # store their indices in idx
     es[idx]  # print the words indexed with idx
   #+end_src

   #+RESULTS:
   : [1] TRUE
   : [1] "de"    "del"   "donde" "desde"

+* TODO Week 9: Word clouds
#+attr_latex: :width 400px
[[../img/clouds.png]]

- Finish TDM and DTM
- Fun with ChatGPT in R
  
- *Word clouds and more interesting visuals:*
  1) Frequent terms with ~tm~ and ~qdap~
  2) Word cloud visualizations
  3) Common word visualization
  4) Word clustering

* TODO Fun with R: New ChatGPT API - use bot in R
#+attr_latex: :width 400px
[[../img/chatgpt.jpg]]

OpenAI has opened the ChatGPT API, and we can access it from within R,
courtesy of Rasmus Bååth (2023):
1) [[https://chat.openai.com/auth/loginLinks][Register with OpenAI here]] (use your Lyon GMail account)
2) Get your secret ~api_key~ [[https://platform.openai.com/account/api-keys][here from OpenAI]] (you need to register)
3) [[https://gist.githubusercontent.com/rasmusab/c25badf55f5dacee14ab13834798d3ef/raw/e3af90fc32d91c974cd2ec9ddb2f7bf52e992cff/chat-gtp-api-call.R][Add this code]] to your ~~/.Rprofile~ file (including the secret key)
4) Install ~httr~ and ~stringr~
5) Now the function ~ask_chatgpt~ will be available to you (GitHub typo)

[[https://www.sumsar.net/blog/call-chatgpt-from-r/][From this post]], you can also learn how to clean the messages from
ChatGPT using standard text mining functions.


* TODO Week 10: Case study - HR analytics
* TODO Week 11: Polarity scoring
* TODO Week 12: Subjectivity lexicons with ~tidytext~
* TODO Week 13: Visualizing sentiment
* TODO Week 14: Cast study - Airbnb reviews
* TODO Week 15: Project presentations 
* References

Rasmus Bååth (3 March 2023). Call ChatGPT (or really any other API)
from R. URL: [[https://www.sumsar.net/blog/call-chatgpt-from-r/][www.sumsar.net]]

* Footnotes

[fn:3] No /assessment bias:/ independent judgements, diverse
understanding, local knowledge and tabulated results.

[fn:2] Structured data is data in rectangular structures (e.g. tables)
whose rows are records or observations, and whose columns are feature
vectors or attributes. A vector of length N can also be printed as a
rectangular structure of dimension N x 1 (N rows, 1 column):
#+begin_src R
  vec <- c("jim", "joe", "jan", "bud")
  vec
  matrix(vec)
  matrix(vec,nrow=1)
#+end_src

#+RESULTS:
: [1] "jim" "joe" "jan" "bud"
:      [,1]
: [1,] "jim"
: [2,] "joe"
: [3,] "jan"
: [4,] "bud"
:      [,1]  [,2]  [,3]  [,4]
: [1,] "jim" "joe" "jan" "bud"

[fn:1] Proposal, literature review, presentation or essay, IMRaD
(Introduction, Method, Results and Discussion) structure.
